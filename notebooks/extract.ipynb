{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "derived-mining",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas\n",
    "#%pip install requests\n",
    "#%pip install googlemaps\n",
    "#%pip install pyth\n",
    "#%pip install pendulum\n",
    "#%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "import pyth\n",
    "import pandas as pd\n",
    "import googlemaps\n",
    "import requests\n",
    "import pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1cd391-db77-4971-ba36-d5170bb810f7",
   "metadata": {},
   "source": [
    "Add Path to `DATA` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80e497-e5d1-4170-9ef8-df7a1fe96754",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Volumes/GoogleDrive/My Drive/NEARIT/DATA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-lloyd",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1. Export the list to text edit\n",
    "2. `textutil -convert txt file.rtf` to transform into text\n",
    "2. Import the list into Python\n",
    "3. Parse the list into a readable format\n",
    "4. Pass each result into the google maps API\n",
    "5. Load the results into Postgres\n",
    "6. Transformation and computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-audience",
   "metadata": {},
   "source": [
    "Complete the mapping with the name of the city, the coordinates indicating the center of the city and the name used to call \"restaurant\" in the local language.\n",
    "We will insert the coordinates within the maximum radius of serach of the place, while we will use the translation of the word \"restaurant\" to add it to the serach query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6181f31-49cc-47e8-82f5-cf0d64669114",
   "metadata": {},
   "source": [
    "## Compare staging and production\n",
    "\n",
    "After comparing, move new files to development\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16737f84-41d4-4ef1-9d90-3dcd886ff6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get previous saved master list\n",
    "\n",
    "# Read folder\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "env_staging = \"staging\"\n",
    "env_development = \"development\"\n",
    "env_production = \"production\"\n",
    "\n",
    "staging_dir = os.path.join(PATH, \"recommendations\", env_staging)\n",
    "development_dir = os.path.join(PATH, \"recommendations\", env_development)\n",
    "production_dir = os.path.join(PATH, \"recommendations\", env_production)\n",
    "\n",
    "# Read name files production and staging and find difference\n",
    "staging_files = os.listdir(staging_dir)\n",
    "production_files = os.listdir(production_dir)\n",
    "\n",
    "new_files = set(staging_files) - set(production_files)\n",
    "if len(new_files) == 0:\n",
    "    print(\"No new files in staging\")\n",
    "else:\n",
    "    print(f\"Found {len(new_files)} in staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d142592-68ff-472a-b715-5fc7e3f36fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move new files to development\n",
    "for f in new_files:\n",
    "    shutil.move(os.path.join(staging_dir, f), development_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678e676-1db1-4839-af08-a8a8952a69b0",
   "metadata": {},
   "source": [
    "## Extract and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f737205-7d7c-4a19-8aa2-3e8f8cc90f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from striprtf.striprtf import rtf_to_text\n",
    "\n",
    "development_files = os.listdir(development_dir)\n",
    "for f in development_files:\n",
    "    filename, file_extension = os.path.splitext(f)\n",
    "    if f.endswith(\".rtf\"):\n",
    "        with open(os.path.join(development_dir, f)) as f:\n",
    "            lines = f.read()\n",
    "            txt = rtf_to_text(lines)\n",
    "            w = open(f\"{filename}.txt\", \"x\")\n",
    "            w.write(txt)\n",
    "            w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-navigation",
   "metadata": {},
   "source": [
    "### Get files and clean all lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def get_files_path(data_folder_path: str, endswith = \".rtf\") -> None:\n",
    "    \"\"\"Get the file path for each file within the data_folder\"\"\"\n",
    "    directory = PATH+data_folder_path\n",
    "    files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if str(file).endswith(endswith):\n",
    "            files.append(PATH+data_folder_path+\"/\"+file)\n",
    "    return files\n",
    "                    \n",
    "def convert_rtf_to_txt(data_folder_path: str, files: str) -> None:\n",
    "    for file in files:\n",
    "        if file.endswith(\".rtf\"):\n",
    "            try:\n",
    "                query = f\"textutil -convert txt '{file}'\"\n",
    "                subprocess.run(query, cwd=f\"./{data_folder_path}\")\n",
    "            except:\n",
    "                print(\"Something went wrong\")\n",
    "        else:\n",
    "            print(f\"{file} is not rtf\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a67eb0-0a1d-492d-bed0-f62724f90609",
   "metadata": {},
   "source": [
    "Use the following to convert to .txt `textutil -convert txt *.rtf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8c6a1-b273-4dfa-b846-1021d15d88d7",
   "metadata": {},
   "source": [
    "### Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbd101-c715-46ff-85d0-c2819bce6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_paths = get_files_path(data_folder_path = \"data\", endswith=\".txt\")\n",
    "print(f\"Found and read {len(files_paths)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e9596-2ce9-41bd-95f2-3a2c3a961dfe",
   "metadata": {},
   "source": [
    "### Get city Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33d6e7-b8a9-4643-9a75-f77775aa8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "CITIES_MAPPING = pd.read_csv(\"mappings/mappings.csv\")\n",
    "print(CITIES_MAPPING)\n",
    "CITIES_MAPPING[\"City\"] = CITIES_MAPPING[\"City\"].str.lower()\n",
    "CIRCLE_SIZES = {\"small\": 3, \"normal\": 5, \"large\": 10, \"very_large\": 50, \"crazy_large\": 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83385fb8-6ee4-410c-bbd8-cf330bf023fa",
   "metadata": {},
   "source": [
    "### Utils to Clean lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9f3fc-4fde-487e-90a1-0dd9f80412b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list_txt(files: List[str]) -> List: \n",
    "    \"\"\"Read and cleans the list of files.\"\"\"\n",
    "    res = []\n",
    "    for file_path in files:\n",
    "        if isinstance(file_path, str):\n",
    "            try:\n",
    "                cleaned_file = read_and_clean_file(file_path)\n",
    "                res.append(cleaned_file)\n",
    "            except:\n",
    "                print(f\"Cleaning file {file_path}\")\n",
    "                raise\n",
    "    return res\n",
    "            \n",
    "def read_and_clean_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Read and clean single file/list.\n",
    "    \n",
    "    Uses the first entry as city name and appends to each row the word representing\n",
    "    \"restaurant\" in the local language.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        path, title = os.path.split(file_path)\n",
    "        title_cleaned = clean_title(title=title)\n",
    "        city_name = get_city_name(title=title_cleaned)\n",
    "        recommender = get_recommender(title=title_cleaned)\n",
    "        list_category = get_list_category(title=title_cleaned)\n",
    "        res = []\n",
    "        for line in lines[1:]:\n",
    "            line = clean_line(line=line)\n",
    "            place, comment = get_comment(line)\n",
    "            place = clean_place(place)\n",
    "            if place == \"\":\n",
    "                continue\n",
    "            \n",
    "            # City information\n",
    "            row = CITIES_MAPPING.loc[CITIES_MAPPING['City'] == city_name]\n",
    "            loc_coord = add_coordinates(row[\"size\"].values[0], row[\"Center\"].values[0])\n",
    "            loc_rest = row[\"Restaurant_lang\"].values[0]\n",
    "            loc_lang = row[\"lang\"].values[0]\n",
    "            place = add_localized_restaurant(place, loc_rest)            \n",
    "            \n",
    "            # Create dict\n",
    "            place_dict = {\n",
    "                \"RecommendedPlace\": place,\n",
    "                \"RecommendedComment\": comment,\n",
    "                \"RecommendedCity\": city_name,\n",
    "                \"RecommendedCateogory\": list_category,\n",
    "                \"EnrichedLocation\": loc_rest,\n",
    "                \"EnrichedCoordinates\": loc_coord,\n",
    "                \"EnrichedLanguage\": loc_lang,\n",
    "                \"Recommender\": recommender,\n",
    "            }\n",
    "            res.append(place_dict)\n",
    "        return res\n",
    "    \n",
    "def get_city_name(title: str) -> str:\n",
    "    try:\n",
    "        res = title.split(\"-\")[1].strip()\n",
    "        return res.split(\".txt\")[0]\n",
    "    except:\n",
    "        print(\"City not found in file title\")\n",
    "        raise\n",
    "\n",
    "def get_recommender(title: str) -> str:\n",
    "    try:\n",
    "        res = title.split(\"-\")[0].strip()\n",
    "    except:\n",
    "        print(\"Recommender name not found in file title\")\n",
    "        raise\n",
    "    return res\n",
    "\n",
    "def get_list_category(title: str) -> str:\n",
    "    try:\n",
    "        res = title.split(\" - \")[2].strip()\n",
    "        return res.split(\".txt\")[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def add_coordinates(size, center):\n",
    "    size_adjustment = CIRCLE_SIZES[size]\n",
    "    res = \"circle:\"+str(size_adjustment)+\"@\"+str(center)\n",
    "    return res\n",
    "    \n",
    "def get_comment(line: str) -> str:\n",
    "    for i in [\"|\", \"(\", \"-\", \":\"]:\n",
    "        if i in line[3:]:\n",
    "            splitted_line = line.split(i)\n",
    "            comment = splitted_line[1].replace(\")\", \"\").replace(\"-\", \"\").strip()\n",
    "            place = splitted_line[0]\n",
    "            return (place, comment)\n",
    "    return (line, None)\n",
    "    \n",
    "def add_localized_restaurant(line: str, localized_restaurant) -> str:\n",
    "    for i in [\"cafÃ©\", \"bar\"]:\n",
    "        if i in line.lower():\n",
    "            return line\n",
    "    return line + f\" {localized_restaurant}\"\n",
    "\n",
    "def clean_place(place: str) -> str:\n",
    "    res = place.strip().replace(\"-\", \"\").strip()\n",
    "    return res\n",
    "\n",
    "def clean_title(title: str) -> str:\n",
    "    res = title.strip().replace(\";\", \"\").replace(\":\", \"\").lower()\n",
    "    return res\n",
    "\n",
    "def clean_line(line: str) -> str:\n",
    "    cleaned_line = line.replace(\"\\n\", \"\").replace(\";\", \"\").strip()\n",
    "    return cleaned_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17452767-3fa1-46c3-be09-91dffecc2be2",
   "metadata": {},
   "source": [
    "### Clean lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718edaf1-96a5-4bfd-bfc1-faf5e1b8926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_lists = clean_list_txt(files_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd791c6d-475c-4ca9-bdd2-d014e27697ba",
   "metadata": {},
   "source": [
    "### Convert to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec2faa-03df-48a6-b806-900b3542309d",
   "metadata": {},
   "source": [
    "Quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427c887-e177-4c1e-883c-e9a626f59bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"RecommendedCity\"] == \"\"]\n",
    "df[df[\"RecommendedPlace\"] == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efa752-57fb-4f89-9f4e-9e2492b8c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists = sum(cleaned_lists, [])\n",
    "df = pd.DataFrame(all_lists)\n",
    "# Data check used later\n",
    "new_lenght = df.shape[0]\n",
    "df[\"Key\"] = df[[\"RecommendedPlace\", \"RecommendedCity\"]].apply(\"_\".join, axis=1)\n",
    "df.head()\n",
    "# Save to Excel using today's date (will be overwritten if exists!)\n",
    "#date = pendulum.today().to_date_string()\n",
    "#print(date)\n",
    "#df.to_excel(f\"extract/{date}_not_enriched.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31bf89-e7ee-4d6d-a6fa-10cb8fb781ea",
   "metadata": {},
   "source": [
    "### Enrich place information using Google Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7286e-542d-4f1a-860b-4bf5645a3b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tqdm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f7893-3d68-4c73-8a22-3587d7c3b453",
   "metadata": {},
   "source": [
    "Read last version of complete database, slice new df with just the new values, based on:\n",
    "\n",
    "- `RecommendedPlace` \n",
    "- `RecommendedCity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d3f68-73c4-4710-8ce4-2cd53698330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous = pd.read_pickle(f\"extract/2022-01-16_raw_df.pkl\")\n",
    "df_previous[\"Key\"] = df_previous[[\"RecommendedPlace\", \"RecommendedCity\"]].apply(\"_\".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f393dae-8943-491c-94ff-9d821b33019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_diff = set(df.Key).difference(df_previous.Key)\n",
    "where_diff = df.Key.isin(key_diff)\n",
    "df_diff = df[where_diff]\n",
    "df_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a3e50-fd99-4c2f-a2e1-c36876007b4d",
   "metadata": {},
   "source": [
    "Now enrich only the differences of the dfs using Google Maps (to waste less calls and improve the speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48fd1a9-732f-4a2e-97e6-ff7da424ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_key = \"AIzaSyCSwOYn7Dhm4HEaevDPD2Q7jQWAT6ii4sM\"\n",
    "gmaps = googlemaps.Client(key=google_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb8517-bcbd-4322-872d-8fa76527ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps.distance_matrix(destinations=[\"76X6PJV3%2B66\"],origins=[\"30.0327738,-90.0226477\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_places(row: str) -> Dict:\n",
    "    time.sleep(0.2)\n",
    "    res = gmaps.find_place(\n",
    "            input=row[\"RecommendedPlace\"],\n",
    "            fields=[\"business_status\", \"name\", \"plus_code\", \"formatted_address\", \"price_level\", \"types\", \"rating\", \"user_ratings_total\", \"permanently_closed\"], # last three more expensive\n",
    "            input_type=\"textquery\", \n",
    "            location_bias=row[\"EnrichedCoordinates\"],\n",
    "            language=row[\"EnrichedLanguage\"],\n",
    "    )\n",
    "    try:\n",
    "        return pd.Series([\n",
    "            res.get(\"candidates\")[0].get(\"business_status\"),\n",
    "            res.get(\"candidates\")[0].get(\"formatted_address\"),\n",
    "            res.get(\"candidates\")[0].get(\"name\"),\n",
    "            res.get(\"candidates\")[0].get(\"plus_code\").get(\"global_code\"),\n",
    "            res.get(\"candidates\")[0].get(\"price_level\"),\n",
    "            res.get(\"candidates\")[0].get(\"rating\"),\n",
    "            res.get(\"candidates\")[0].get(\"types\"),\n",
    "            res.get(\"candidates\")[0].get(\"user_ratings_total\"),\n",
    "            res.get(\"status\"),\n",
    "        ])\n",
    "    except:\n",
    "        print(f'{row[\"RecommendedPlace\"]} was not found')\n",
    "        return pd.Series([\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50062a87-3e9c-4b33-8109-f4cc81834941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_diff[['business_status', 'formatted_address', 'name', \"global_code\", \"price_level\", \"rating\", \"types\", \"user_ratings_total\", \"status\"]] = df_diff.progress_apply(lambda row: find_places(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29035764-bccb-4d79-b9f2-f22f0b79df21",
   "metadata": {},
   "source": [
    "### Quality check\n",
    "Eliminate city if it's more than 200km "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe0997-1141-43a5-bb5c-e34138d2bd3b",
   "metadata": {},
   "source": [
    "### Merge difference with database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9d1c5-a7af-4b12-b638-b2efc34c2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab2903-e57b-47e3-b8f9-714479c2c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a41c9-a5d5-432b-8cff-c3a7aad131bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous = df_previous.append(df_diff, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90682195-a4c9-4d6b-aeca-1f507dd6a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data check - Should be TRUE\n",
    "print(new_length == df_previous.shape[0])\n",
    "print(df_previous.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e9ec2-a348-4b40-b047-a9345834a38e",
   "metadata": {},
   "source": [
    "### Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1def809-0139-4e3c-abac-05a3ee2ee9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pendulum\n",
    "\n",
    "date = pendulum.today().to_date_string()\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ebe4d7-a011-46a4-ac3c-20a4d5fb0095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw dataframe\n",
    "df.to_pickle(f\"extract/{date}_raw_df.pkl\")\n",
    "\n",
    "# Save Excel\n",
    "df.to_excel(f\"extract/{date}_raw_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a8aeb-3c83-4a6e-8093-8212bfd88014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "407ad6f977447d2e7491b3a524b413ddabef0ef42c8af9f2f5af5edb32041277"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
